{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "_include_('curriculum_vqa')\n",
    "\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed);\n",
    "\n",
    "from cvqa import datasets, models, trainers, viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvqa.curriculum import VQAInstanceDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.data import Dictionary\n",
    "import torch.nn.functional as F\n",
    "\n",
    "samples = []\n",
    "for s in VQAInstanceDistribution().sample_dataset(images=10, prompts_per_image=5):\n",
    "    samples.append({\n",
    "        'prompt': s['prompt'] + ' ans = ' + s['target'],\n",
    "        'target': s['target']\n",
    "    })\n",
    "    \n",
    "dataset = datasets.SimpleDataset(samples)\n",
    "vocab = dataset.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvqa import fairseq_misc\n",
    "model = fairseq_misc.build_transformer(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MyTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, enc, dec):\n",
    "        super().__init__()\n",
    "        self.enc = enc\n",
    "        self.dec = dec\n",
    "        \n",
    "    def forward(self, src_tokens, src_lengths, prev_output_tokens):\n",
    "        encoder_out = self.enc(src_tokens, None)\n",
    "        decoder_out = self.dec(prev_output_tokens, encoder_out=encoder_out)\n",
    "        return decoder_out\n",
    "    \n",
    "tokens_embed = fairseq_misc.build_embedding(vocab, 16)\n",
    "encoder = fairseq_misc.build_vqa_encoder(vocab, tokens_embed)\n",
    "decoder = fairseq_misc.build_decoder(vocab, tokens_embed)\n",
    "model = MyTransformer(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "import statistics\n",
    "\n",
    "def model_forward(model, sample):\n",
    "    src_tokens = sample['prompt']\n",
    "    targets = sample['target']\n",
    "    src_lengths = None\n",
    "    \n",
    "    B = src_tokens.shape[0]\n",
    "    prev_output_tokens = torch.zeros(B, 1, dtype=torch.int64)\n",
    "    model_out = model(src_tokens, src_lengths, prev_output_tokens)\n",
    "    logits = model_out[0]\n",
    "    \n",
    "    logits = logits.view(-1, logits.size(-1)) \n",
    "    targets = targets.flatten()\n",
    "    return logits, targets\n",
    "\n",
    "\n",
    "def evaluate(model, dataset, ignore_index=None, iter_lim=None):\n",
    "    dloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, sample in enumerate(dloader):\n",
    "            if iter_lim is not None and i >= iter_lim:\n",
    "                break\n",
    "\n",
    "            logits, y_true = model_forward(model, sample)\n",
    "\n",
    "            _, y_pred = torch.max(logits.data, -1)\n",
    "\n",
    "            if ignore_index is not None:\n",
    "                mask = y_true.ne(self.ignore_index)\n",
    "                y_true = y_true[mask]\n",
    "                y_pred = y_pred[mask]\n",
    "\n",
    "            correct += (y_pred == y_true).sum()\n",
    "            total += y_true.size(0)\n",
    "\n",
    "        return float(correct) / float(total)\n",
    "    \n",
    "    \n",
    "def train(model, dataset, optim, num_epochs=2):\n",
    "    dloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    train_loss = []\n",
    "    with tqdm(range(num_epochs)) as prg_train:\n",
    "        for epoch in prg_train:\n",
    "            for sample in dloader:\n",
    "                model.train()\n",
    "                optim.zero_grad()\n",
    "\n",
    "                logits, targets = model_forward(model, sample)\n",
    "                loss = crit(logits, targets)\n",
    "\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "                train_loss.append(loss.item())\n",
    "                running_mean_loss = statistics.mean(train_loss[-min(len(train_loss), 100):])\n",
    "                status_str = f'[epoch={epoch}] loss: {running_mean_loss:.3f}'\n",
    "                prg_train.set_description(status_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[epoch=249] loss: 0.639: 100%|██████████| 250/250 [00:07<00:00, 35.17it/s]\n"
     ]
    }
   ],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "train(model, dataset, optim, num_epochs=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "curriculum_vqa",
   "language": "python",
   "name": "curriculum_vqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
